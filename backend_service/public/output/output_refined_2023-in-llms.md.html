<hr/>

<hr/>
<h1>2023年，开源大语言模型之年</h1>

<p>2023年，大语言模型（LLMs）引起了公众的极大兴趣。现在大多数人对这些模型有了一定的了解，关于开放源码与封闭源码的公共辩论也吸引了广泛的关注。在Hugging Face，我们对开源模型充满了兴趣，因为它们使研究可重复，赋予社区参与AI模型开发的能力，便于更轻松地审查模型的偏见和局限性，并通过支持检查点重用来降低该领域的整体碳排放（以及其他<a href="https://huggingface.co/papers/2302.04844">诸多好处</a>）。</p>
那么，让我们回顾一下这一年在开放大语言模型方面的情况吧！
<p><em>为了使本文档的长度保持在可管理的范围内，我们不会讨论代码模型。</em></p>
<h2>🍜 预训练大语言模型的配方</h2>

<p>首先，如何获得一个大语言模型？（如果你已经了解，可以跳过这一节！）</p>
<p>模型<strong>架构</strong>（其代码）描述了具体的实现和数学形态，包括所有参数的列表及其与输入的交互方式。目前，性能卓越的大语言模型大多数是“仅解码器”Transformer架构的变体（更多细节请参见<a href="https://huggingface.co/papers/1706.03762">原始Transformer论文</a>）。</p>
<p><strong>训练数据集</strong>包含了模型训练所用的所有例子和文档（即参数通过它们学习），因此反映了学习到的特定模式。通常，这些文档包含以文本形式存在的内容，无论是自然语言（如法语、英语、中文）、编程语言（如Python、C），还是任何可以表示为文本的结构化数据（如markdown或latex中的表格、方程式等）。</p>
一个<strong>分词器</strong>定义了如何将训练数据集中的文本转换为数字（因为模型是一个数学函数，因此需要数字作为输入）。分词通过将文本转换为称为 tokens 的子单元来完成（这些子单元可以是单词、子词或字符，具体取决于分词方法）。分词器的词汇量指示其识别多少不同的 token，通常在 32,000 到 200,000 之间。数据集的大小通常通过在分割成这些独立的“原子”单位后所包含的<strong>token 数量</strong>来衡量，目前的范围从几千亿个 token 到几万亿个 token！
<p><strong>训练超参数</strong>用于定义模型如何进行训练。每个新样本应调整多少参数来适应？模型的更新速度应该多快？</p>
一旦选择了这些参数，你只需具备 1) 大量的计算能力来训练模型，以及 2) 能胜任且友好的人来运行和监控训练。训练的过程包括实例化架构（在用于训练的硬件上创建矩阵），并使用上述超参数在训练数据集上运行训练算法。最终结果是得到一组模型的<strong>权重</strong>。这些是模型学习后的参数，也是多数人在提到访问开放的预训练模型时的意思。这些权重可用于<strong>推理</strong>，即对新输入进行预测，比如生成文本。
预训练的大语言模型在完成预训练后，还可以针对特定任务进行专门化或适应性调整，特别是当其权重被公开发布时。这些模型通过称为<strong>微调</strong>的过程，被用作用例和应用程序的起点。微调是指在不同的——通常更专业并且规模较小的数据集上，对模型应用额外的训练步骤，以优化其用于特定应用。尽管在计算能力方面，这一步会产生一定成本，但通常比从头开始训练一个模型的花费要少得多，无论是在经济上还是环境上。这也是为什么高质量的开源预训练模型非常吸引人的原因之一，因为社区可以在有限的计算预算下，免费使用并在此基础上进行构建。
<h2>🗝️ 2022，从规模竞赛到数据竞赛</h2> <p>在2023年之前，社区有哪些公开的模型可供使用？</p>
直到 2022 年初，机器学习领域的趋势是模型越大（即参数越多），其性能就越好。特别是，模型在超过特定规模阈值后，似乎具备了跳跃性的能力，这两个概念被称为<code>涌现能力</code>和<code>扩展定律</code>。2022 年发布的开源预训练模型家族基本都遵循这一范式。1. <a href="https://huggingface.co/papers/2211.05100">BLOOM</a>（BigScience 大型开放科学开放访问多语言模型）BLOOM 是由 BigScience 发布的一系列<a href="https://huggingface.co/bigscience/bloom">模型</a>，这一合作项目由 Hugging Face 协调，汇集了来自 60 个国家和 250 个机构的 1000 名研究人员，并与法国组织 GENCI 和 IDRIS 合作。这些模型使用仅解码器的 Transformer，进行了微小的修改（包括嵌入后归一化[^1]和使用 ALiBi 位置嵌入[^2]）。该系列中最大的模型拥有 1760 亿参数，训练于 46 种人类语言和 13 种编程语言的 3500 亿 tokens 的多语言数据上。大部分训练数据已发布，并提供了其来源、整理和处理的详细信息。它是目前最大的开源大规模多语言模型。
OPT（开放预训练的 Transformer）
OPT 模型家族由 Meta 发布。这些模型采用仅解码器的 Transformer 架构，沿用了 GPT-3 论文中的技巧（特定的权重初始化，预归一化），并对注意力机制进行了若干改进（交替使用密集注意力层和局部带状注意力层）。这一家族中规模最大的模型参数达 1750 亿，使用的数据量达 1800 亿 Token，数据主要来自公共来源，如书籍、Reddit 社交数据、新闻、维基百科以及其他各种互联网资源。通过编码优化，这个模型家族的性能可与 GPT-3 模型相媲美，并减少了计算强度。
<ol><li><a href="https://huggingface.co/papers/2210.02414">GLM-130B</a>（通用语言模型）<a href="https://huggingface.co/THUDM/glm-roberta-large">GLM-130B</a>由清华大学与智谱AI发布。此模型采用了完整的Transformer架构，并进行了一些调整（使用DeepNorm的后层归一化、旋转嵌入）。1300亿参数的模型在4000亿个英语和中文互联网数据（包括The Pile、悟道语料库及其他中文语料库）上进行了训练，其性能与 GPT-3 模型相当。</li></ol>
<ol><li>更小或更专业的开放大语言模型
较小的开源模型也被发布，主要用于研究目的：Meta 发布了 <a href="https://huggingface.co/papers/2211.09085">Galactica</a> 系列，其大语言模型最高可达 <a href="https://huggingface.co/facebook/galactica-120b">120B</a> 个参数，预训练基于 106B 个科学文献 Token；而 EleutherAI 发布了 <a href="https://huggingface.co/EleutherAI/gpt-neox-20b">GPT-NeoX-20B</a> 模型，这是一种完全开源的（架构、权重、数据均包括）解码 Transformer 模型，训练于 5000 亿个 Token（使用 RoPE 并对注意力机制和初始化进行了一些改进），为科学研究提供完整的工具。</li></ol>
这些庞大的模型令人兴奋，但运行成本也非常高！在进行推理（从模型计算预测）时，模型需要加载到内存中，而一个拥有1000亿参数的模型通常需要220GB的内存来加载（我们将在下文解释这一过程），这对于大多数机构和从业者来说都非常庞大且难以负担！
然而，在2022年3月，DeepMind发布了一篇<a href="https://huggingface.co/papers/2203.15556">新论文</a>，研究了在给定计算预算下，token与模型参数的最佳比例是什么。换句话说，如果你的模型训练预算为X，模型和数据的规模应该如何搭配？作者发现，总的来说，对于用于大语言模型的平均计算预算，模型应该更小，但需要在显著更多的数据上进行训练。他们开发的Chinchilla模型（非开源）具有700亿参数（是上述模型大小的三分之一），但在1.4万亿个token数据上进行训练（数据量是其他模型的3到4倍）。其性能与更大规模的开源和闭源模型相似或更优。
这种范式转变，尽管可能在封闭的实验室中已为人所知，但仍然让开放科学界感到震惊。
<hr/>
<h1>2023年，开放大语言模型之年</h1>

<p>2023年，公众对大语言模型（LLMs）的兴趣激增。如今，大多数人已经对这些模型及其可能性有了一定的了解，关于开放源模型与闭源模型的公众争论也广泛传播。在Hugging Face，我们对开放模型非常关注，因为它们使得研究可以重复进行，赋予社区参与AI模型开发的能力，便于更容易审查模型的偏见和局限性，并通过支持重用检查点（以及<a href="https://huggingface.co/papers/2302.04844">许多其他好处</a>），降低我们领域的整体碳排放。</p>
让我们回顾一下开放大语言模型领域的这一年吧！
<p><em>为了保持文件篇幅可控，我们不会涉及代码模型。</em></p>
<h2>🍜 预训练大语言模型的配方</h2>

<p>首先，如何获得一个大语言模型？（如果你已经知道，可以跳过这一部分！）</p>
<p>模型的<strong>架构</strong>（其代码）描述了其具体实现和数学形状：它包括所有参数的列表，以及它们如何与输入交互。目前，大多数高性能的大语言模型都是“仅解码器”Transformer架构的变体（更多细节见<a href="https://huggingface.co/papers/1706.03762">原始transformers论文</a>）。</p>
<p><strong>训练数据集</strong>包含所有用于模型训练的示例和文档（即参数的学习过程），因此，特定模式便被学到。大多数情况下，这些文档包含文本，可能是自然语言（如法语、英语、中文）、编程语言（如Python、C）或任何可以用文本表达的结构化数据（如markdown或latex中的表格、方程式等）。</p>
<p><strong>分词器</strong>定义了如何将训练数据集中的文本转换为数字（因为模型是一个数学函数，因此需要数字作为输入）。分词是通过将文本转换为被称为Token的子单元来完成的（具体是单词、子词还是字符，取决于分词方法）。分词器的词汇量大小表示它已知的不同Token的数量，通常在32k到200k之间。一个数据集的大小通常是通过它所包含的<strong>Token数量</strong>来衡量的，一旦被拆分为这些独立的“原子化”单元序列时，如今的范围从数千亿个Token到数万亿个Token！</p>
<p><strong>训练超参数</strong>用于定义模型的训练方式。每个新样例应该调整多少参数？模型更新的速度应该多快？</p>
一旦选择了这些参数，您只需要1）大量计算能力来训练模型，2）有能力且友善的人员来运行和监控训练。训练过程包括实例化架构（在用于训练的硬件上创建矩阵）并利用上述超参数在训练数据集上执行训练算法。结果是得到一组模型<strong>权重</strong>，这些是模型学习后的参数，并且是大多数人讨论访问开放预训练模型时所指的内容。这些权重可用于<strong>推理</strong>，即对新输入进行预测，例如生成文本。
预训练的大语言模型在预训练之后也可以针对特定任务进行专门化或适配，特别是当权重公开发布时。然后，它们通过一个称为<strong>微调</strong>的过程，被用作实际应用的起点。微调涉及对模型在一个不同的——通常更专门化且更小的——数据集上进行额外的训练步骤，以优化其在特定应用上的表现。尽管这个步骤需要消耗一定的计算资源，但通常比从头开始训练模型要经济得多，无论是在经济成本还是环保角度。因此，高质量的开源预训练模型非常有吸引力，因为即使社区的实践者只有有限的计算预算，它们也可以被自由利用和构建。
<h2>🗝️ 2022年，从规模竞赛转向数据竞赛</h2> <p>2023年之前，社区有哪些开放模型可用？</p>
到2022年初，机器学习领域的趋势是，模型越大（即参数越多），其性能就越好。特别是，超过某些规模临界点的模型在功能上会有显著提升，这两个概念被称为“涌现能力”和“缩放法则”。2022年发布的开源预训练模型家族大多遵循这一范式。

1. [BLOOM](https://huggingface.co/papers/2211.05100)（BigScience 大规模开放科学开放访问多语言模型）
BLOOM是由BigScience发布的一个模型家族，集结了来自60个国家和250个机构的1000名研究人员的合作，由Hugging Face协调，并与法国的GENCI和IDRIS组织合作。这些模型采用仅解码器的Transformer, 并且进行了轻微修改（如嵌入后归一化[^1]和使用ALiBi位置嵌入[^2]）。这个家族中最大的模型是一个拥有1760亿参数的模型，其训练数据量达到3500亿token，覆盖了46种人类语言和13种编程语言。大部分训练数据已公开，并发布了其来源、筛选和处理的详细信息。它是迄今为止最大规模的开源多语言模型。
<ol><li><a href="https://huggingface.co/papers/2205.01068">OPT</a> (开放预训练Transformer)
OPT<a href="https://huggingface.co/facebook/opt-66b">模型</a>家族是由Meta公司发布的。这些模型采用的是仅解码器的Transformer架构，遵循GPT-3论文中的一些技巧（特定的权重初始化，预归一化），并对注意力机制进行了一些更改（交替使用密集和局部带状注意力层）。该家族中最大的模型拥有1750亿参数，基于大部分公共来源的数据（包括书籍、通过Reddit获取的社交数据、新闻、维基百科及其他各种互联网来源）训练了1800亿个tokens。该模型家族的性能与GPT-3模型相当，并通过编码优化来降低计算强度。</li></ol>
<ol><li><a href="https://huggingface.co/papers/2210.02414">GLM-130B</a>（通用语言模型）由<a href="https://huggingface.co/THUDM/glm-roberta-large">清华大学和智谱.AI</a>发布。它采用了完整的Transformer架构，并进行了某些更改（使用DeepNorm的后层归一化，旋转嵌入）。这个包含1300亿参数的模型在4000亿个英文和中文互联网数据（包括The Pile、悟道语料库及其他中文语料库）上进行了训练。其性能也与GPT-3模型相当。</li></ol>
<ol><li>更小规模或更专业化的开放式大语言模型也已发布，主要用于研究目的：Meta公司发布了<a href="https://huggingface.co/papers/2211.09085">Galactica</a>系列，其大语言模型的参数规模达到<a href="https://huggingface.co/facebook/galactica-120b">1200亿</a>个，并在1060亿个科学文献Token上进行了预训练；EleutherAI发布了<a href="https://huggingface.co/EleutherAI/gpt-neox-20b">GPT-NeoX-20B</a>模型，这是一个完全开源的模型（包括架构、权重和数据），属于解码器Transformer模型，在5000亿个Token上进行了训练（使用RoPE技术并对注意力机制和初始化进行了某些修改），提供了一项完整的科学研究装置。</li></ol>
这些大型模型令人兴奋，但运行成本也非常高！在执行推理（从模型计算预测结果）时，模型需要加载到内存中，但一个拥有1000亿参数的模型通常需要220GB的内存来加载（我们将在下面详细解释这个过程），这非常庞大，大多数组织和从业者无法获得如此多的内存！
然而，在2022年3月，DeepMind发布了一篇<a href="https://huggingface.co/papers/2203.15556">新论文</a>，研究在给定的计算预算下，token与模型参数的最佳比例是什么。换句话说，如果你只有X金额用于模型训练，那么模型和数据的大小应分别是多少？作者发现，总体而言，对于当前在大语言模型上所花费的平均计算预算，模型应更小，但需在更多的数据上进行训练。他们的自家模型Chinchilla（非开源）是一个有700亿参数的模型（约为上述模型的三分之一大小），但在1.4万亿token的数据上进行训练（数据量是3到4倍）。其性能与那些更大规模的开源或闭源模型相似或更佳。
<p>这一范式转变，尽管可能早已在封闭的实验室中为人所知，却在开放科学界引起了轰动。</p>
<h2>🌊 2023年，开放发布之年</h2>

<h3>小型大语言模型的崛起</h3>

<p>2023年，解码器风格的Transformer如同潮水般涌现，每月、每周甚至每天都有新的预训练模型发布：2月的LLaMA（由Meta发布），4月的StableLM（由StabilityAI发布）和Pythia（由Eleuther AI发布），5月的MPT（由MosaicML发布），6月的X-GEN（由Salesforce发布）和Falcon（由TIIUAE发布），7月的Llama 2（由Meta发布），8月的StableLM v2（由StabilityAI发布），9月的Qwen（由阿里巴巴发布）和Mistral（由Mistral.AI发布），11月的Yi（由01-ai发布），以及12月的DeciLM（由Deci发布）、Phi-2和SOLAR（由Upstage发布）。</p>
所有这些发布都包含了a) 模型权重（在不同程度开放许可下），以及b) 在较小规模的模型上表现优异（参数在3B到70B之间），因此迅速被社区采纳。几乎所有这些模型都采用了解码器Transformer架构，并进行了多种调整（如ALiBi或RoPE，RMS预归一化，SwiGLU），还有针对注意力功能的一些修改（如Flash-Attention，GQA，滑动窗口）以及不同的代码库实现，用于优化训练或推理速度。这些调整可能在某种程度上影响性能和训练速度；然而，由于所有架构已随权重一同公开，剩下的核心差异在于训练数据和模型许可。
这个系列的第一个模型家族是由 Meta AI 发布的<a href="https://huggingface.co/papers/2302.13971">LLaMA</a>家族。研究人员的明确目标是在给定的计算预算下，训练出一组不同大小且性能最佳的模型。这是研究团队首次明确决定，不仅要考虑训练预算，还要关注推理成本（即在达到给定性能目标的情况下，运行模型推理所需的成本）。从这一角度，他们选择在更多的数据和更多的步骤上训练更小的模型，因此在模型体积更小的情况下达到了更高的性能（权衡是训练计算效率）。Llama 1 家族中最大的模型是一个拥有 650 亿参数的模型，训练在 1.4 万亿个 token 上，而较小的模型（分别是 60 亿和 130 亿参数）则训练在 1 万亿个 token 上。小型的 130 亿参数的 LLaMA 模型在大多数基准测试中超越了 GPT-3，而最大的 LLaMA 模型在问世时则代表了最先进的水平。不过，这些模型权重以非商业许可发布，限制了社区的广泛采用。
Pythia 模型由开源非营利实验室 Eleuther AI 发布，这是一套不同尺寸的大语言模型，基于完全公开的数据进行训练，旨在帮助研究人员理解大语言模型训练的不同步骤。
<p>MosaicML公司在几个月后发布的<a href="https://www.mosaicml.com/blog/mpt-7b">MPT模型</a>性能相当接近，但其许可允许商业使用，并详细介绍了训练数据的组成。第一个MPT模型是<a href="https://huggingface.co/mosaicml/mpt-7b">7B模型</a>，随后在6月推出了30B版本。这些模型都在1兆个Token的英语和代码数据上进行训练（数据来源于C4、CommonCrawl、The Stack、S2ORC）。</p>
MPT 模型紧接着由 TIIUAE 发布的 Falcon 系列中的 7 和 30B 模型，这些模型在 1 至 1.5T 的英文和代码 tokens 上训练（使用的来源包括 RefinedWeb、Project Gutenberg、Reddit、StackOverflow、Github、arXiv、Wikipedia 等）——当年较晚些时候，还发布了一个庞大的 180B 模型。Falcon 模型、数据及其训练过程的详细信息已在技术报告和后来的研究论文中进行了说明。
StabilityAI发布了一系列继承自GPT-Neo-X模型的StableLM-Base-Alpha模型，这些小型（3B和7B）预训练模型使用了一个基于ThePile构建的实验数据集中的1.5万亿个Token。其后是一个数据组合包括RefinedWeb、RedPajama、ThePile和一些未公开的内部数据集的v2系列。最后，还有一个非常小的3B模型，也就是StableLM-3B-4e1T，并附有一份详细的技术报告。
之前的大多数模型都公开了它们的数据，但从那时起，后续版本几乎没有透露用于训练模型的数据，这使其难以复制。不过，他们仍通过发布的权重为社区成员提供了研究的起点。
夏初，Salesforce推出了X-Gen模型，这是一个拥有70亿参数的模型，在1.5万亿的“自然语言和代码”tokens上进行训练，分几个步骤进行，并遵循数据调度系统（并不是所有数据同时引入模型）。
X-Gen在某种程度上被Meta显然更受关注的全新LLaMA-2系列所掩盖，这个系列的模型从7亿到700亿不等，基于2万亿个来自“公开可用来源”的tokens进行训练，具有宽松的社区许可，并通过广泛的人类偏好微调过程（人类反馈的强化学习）进行对齐。
几个月后，新成立的初创公司 Mistral 发布了首个模型，名为 Mistral-7B，据称是在从"开放网络"中提取的未公开数量的 tokens 上训练的。2023 年底是模型发布的高峰期，Mistral 发布了第二个更大的模型（Mixtral 8x7B），Deci.AI 推出了令人印象深刻的首个模型，称为 DeciLM，还有 Upstage 的更大模型合并，称为 SOLAR，这些模型同样是在未公开数量和来源的数据上训练的。所有这些模型在排行榜和开放基准上都实现了稳定增长。
与此同时，2023年末的一个显著事件是中国训练并公开发布的性能优异的多个模型的兴起。推出了两个英汉双语模型系列：阿里巴巴的Qwen，参数范围从7亿到70亿，训练于2.4万亿个Token上；以及01-AI的Yi，参数范围从6亿到34亿，训练于3万亿个Token上。这些模型的性能不仅在开放LLM排行榜上处于领先地位，而且在一些最具挑战性的基准测试如Skill-Mix中表现优异。另一个在2023年底出现的强劲竞争者是来自DeepSeek AI的DeepSeek编码模型，从零开始在2万亿个Token上训练，其中87%是代码，13%是英语和汉语的自然语言（主要是一种代码模型）。

<h3>对话模型无处不在</h3>

<p>与2022年相比，2023年发布的几乎所有预训练模型都提供了预训练版和经过对话微调的版本，采用了几种现有的方法之一。虽然在2022年及更早的时候已经开发了使模型适应聊天环境的方法，但这些技术在2023年才开始被广泛采用，突显了普通公众对这些聊天模型使用的增加，以及通过与模型对话进行的人为评价（"氛围检测"评价）的增加。在这里，我们详细介绍了一些将预训练模型适应聊天的最知名的方法，但实际上有许多变体！</p>
<strong>基于聊天的微调</strong>是一种监督微调的变体，其中注释数据为聊天数据（类似于多轮对话的数据，就像在社交媒体上看到的那样），并在此基础上对模型进行微调。使用与模型训练时相同的技术：对于解码器Transformer，逐个教模型预测下一个单词（称为自回归方法）。
<p><strong>指令微调</strong> (IFT) 采用相同的方法，但使用指令数据集，这些数据集包含查询类提示词和答案（必要时可以有额外输入）。这些数据集用于教授模型如何执行指令，可以由人类或大语言模型生成。使用大规模模型输出的合成数据集（如从 GPT-4 生成的内容，可能来自指令或用户与模型之间的互动）是实现指令和聊天微调的方法之一。这通常被称为<code>蒸馏</code>，因为这个过程涉及从高性能模型中提取知识，以训练或微调一个较小的模型。</p>
