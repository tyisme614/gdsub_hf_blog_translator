---

---
# 2023年，开源大语言模型之年

2023年，大语言模型（LLMs）引起了公众的极大兴趣。现在大多数人对这些模型有了一定的了解，关于开放源码与封闭源码的公共辩论也赢得了广泛关注。在Hugging Face，我们对开源模型保持着浓厚的兴趣，因为它们使研究具有可重复性，赋予社区参与AI模型开发的能力，便于更容易地审查模型的偏见和局限性，并通过支持检查点重用来降低我们领域的整体碳消耗（以及[许多其他好处](https://huggingface.co/papers/2302.04844)）。
那么，让我们回顾一下开放大语言模型这一年的情况吧！
_为了保持本文档长度可控，我们将不涉及代码模型。_
## 🍜 预训练大语言模型的配方

首先，如何获得一个大语言模型？（如果你已经知道，可以跳过这一节！）
模型**架构**（其代码）描述了其具体的实现和数学形态：它是其所有参数的列表，以及它们与输入的交互方式。目前，性能最好的大语言模型大多是“仅解码器”Transformer架构的变种（更多细节请参见[原始Transformers论文](https://huggingface.co/papers/1706.03762)）。
**训练数据集**包含了用于训练模型的所有示例和文档（即参数的学习过程），因此，包含了学习到的特定模式。大多数情况下，这些文档包含文本，可以是自然语言（例如：法语、英语、中文）、编程语言（例如：Python、C），或者是任何可以表示为文本的结构化数据（例如：markdown 或 latex 中的表格，方程式，......）。
一个**分词器**定义了如何将训练数据集中的文本转化为数字（因为模型是一个数学函数，因此需要数字作为输入）。分词是通过将文本转化为称为 token 的子单元来完成的（这些子单元可以是单词、子词或字符，具体取决于分词方法）。分词器的词汇量指示它知道多少个不同的 token，通常在 32,000 到 200,000 之间。数据集的大小通常通过分成这些具有“原子性”单位的序列后所包含的**token 数量**来衡量，如今的范围从几千亿个 token 到几万亿个 token！
**训练超参数**然后定义模型的训练方式。每个新样本应该改变多少参数以适应？模型应该多快更新？
一旦选择了这些参数，你只需要 1) 大量的计算能力来训练模型，以及 2) 能胜任且友好的人来运行和监控训练。训练本身将包括实例化架构（在用于训练的硬件上创建矩阵）并使用上述超参数在训练数据集上运行训练算法。结果是一组模型**权重**。这些是学习之后的模型参数，也是大多数人在讨论访问开源预训练模型时指的内容。这些权重随后可以用于**推理**，即对新输入进行预测，例如生成文本。
预训练的大语言模型在预训练之后也可以针对特定任务进行专门化或适应性调整，特别是当其权重公开发布时。它们被用作通过名为**微调**的过程来进行用例和应用程序的起点。微调涉及在不同的 —— 通常更专业和更小 —— 数据集上对模型应用额外的训练步骤，以优化其用于特定的应用程序。尽管这一步在计算能力需求方面具有成本，但通常比从头开始训练模型要低得多，无论是在财务上还是环保上。这是高质量开源预训练模型非常有趣的一个原因，因为即使实践者只有有限的计算预算，它们也可以被社区免费使用和构建。
## 🗝️ 2022，从规模竞赛到数据竞赛

在2023年之前，社区有哪些开放的模型可用？
直到 2022 年初，机器学习的趋势是模型越大（即参数越多），其性能就越好。特别是，似乎模型超过特定大小阈值后，其能力出现跳跃，这两个概念被称为`涌现能力` 和`扩展定律`。2022 年发布的开源预训练模型家族大多遵循这一范式。1\. [BLOOM](https://huggingface.co/papers/2211.05100)（BigScience 大型开放科学开放访问多语言模型）BLOOM 是 BigScience 发布的一组[模型](https://huggingface.co/bigscience/bloom)，由 Hugging Face 协调，包括来自 60 个国家和 250 个机构的 1000 名研究人员共同协作，并与法国组织 GENCI 和 IDRIS 合作。这些模型使用仅解码器的 transformers，并进行了微小修改（嵌入后归一化\[^1\] 和使用 ALiBi 位置嵌入\[^2\]）。这个家族中最大的模型是一个拥有 1760 亿参数的模型，在 46 种人类语言和 13 种编程语言的 3500 亿 tokens 多语言数据上进行训练。大部分训练数据已发布，并公布了其来源、整理和处理的详细信息。它是迄今为止最大的开源大规模多语言模型。
OPT（开放预训练的 Transformer） OPT 模型家族由 Meta 发布。这些模型仅使用解码器的 Transformer 架构，遵循 GPT-3 论文中的技巧（特定的权重初始化，预归一化），并对注意力机制进行了一些更改（交替使用密集和局部带状注意力层）。这个家族中最大的模型是一个拥有 175B 参数的模型，基于主要来自公共来源（书籍、社交数据通过 Reddit、新闻、Wikipedia 和其他各种互联网来源）的 180B tokens 数据进行训练。这个模型家族的性能可与 GPT-3 模型媲美，通过编码优化使其计算强度较低。
1. [GLM-130B](https://huggingface.co/papers/2210.02414)（通用语言模型）[GLM-130B](https://huggingface.co/THUDM/glm-roberta-large)由清华大学和智谱AI发布。它使用了一个完整的Transformer架构并进行了一些修改（使用DeepNorm的后层归一化，旋转嵌入）。具有1300亿参数的模型在4000亿个英文和中文互联网数据（The Pile，悟道语料库和其他中文语料库）上进行了训练。其性能也可与GPT-3模型相媲美。
1. 更小或更专业的开放大语言模型（LLM） 更小的开源模型也被发布，主要用于研究目的：Meta 发布了 [Galactica](https://huggingface.co/papers/2211.09085) 系列，大语言模型（LLM）参数最高可达 [120B](https://huggingface.co/facebook/galactica-120b)，基于 106B 个科学文献 Token 进行预训练，而 EleutherAI 发布了 [GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b) 模型，一个完全开源的（包括架构、权重和数据）解码器 Transformer 模型，训练于 500B 个 Token（使用 RoPE 和对注意力机制及初始化的一些修改），为科学研究提供完整的工具。
这些巨大的模型令人兴奋，但运行成本非常高！在进行推理（从模型计算预测）时，模型需要加载到内存中，但一个拥有1000亿参数的模型通常需要220GB的内存来加载（我们将在下文解释这一过程），这非常巨大，对于大多数组织和从业者来说并不容易获取！
然而，在2022年3月，DeepMind 发布了一篇[新论文](https://huggingface.co/papers/2203.15556)，探讨了在给定计算预算下，tokens 与模型参数的最佳比例是什么。换句话说，如果你只有X金额来用于模型训练，模型和数据的相应规模应该是多少？作者发现，总体而言，对于用于大语言模型的平均计算预算，模型应该更小，但要在更多的数据上进行训练。他们自己的模型Chinchilla（非开源）是一个拥有700亿参数的模型（是上述模型大小的三分之一），但训练在1.4万亿个 tokens 的数据上（数据量是3到4倍）。它的性能与更大规模开源和闭源模型相似或更好。
这种范式转变，虽然可能已经在封闭的实验室中为人所知，但仍然使开放科学界感到震惊。
---
# 2023年，开放大语言模型之年

2023年，公众对大语言模型（LLMs）的兴趣激增。现在大多数人对这些模型的概念和用途已有所了解，围绕开放源代码与闭源码的公众辩论也达到了广泛的受众。在Hugging Face，我们对开放模型非常感兴趣，因为它们使研究可再现，赋予社区参与AI模型开发的权限，便于更容易地审视模型的偏见和局限性，并通过支持检查点的重用（等等[众多其他好处](https://huggingface.co/papers/2302.04844)）来降低我们领域的整体碳影响。
让我们回顾一下开放大语言模型领域的这一年吧！
_为了让这份文件长度可控，我们不会研究代码模型。_
## 🍜 预训练大语言模型的配方

首先，如何获得一个大语言模型？（如果你已经知道，可以略过这一部分！）
模型的**架构**（其代码）描述了其具体的实现和数学形状：它是所有参数的列表，以及它们如何与输入交互。目前，大多数高性能的大语言模型都是“仅解码器”Transformer架构的变体（更多细节见[原始transformers论文](https://huggingface.co/papers/1706.03762)）。
**训练数据集**包含模型训练中使用的所有示例和文档（即参数的学习过程），因此，它们所学到的特定模式。大多数情况下，这些文档包含文本，既可以是自然语言（如法语、英语、中文），也可以是编程语言（如Python、C），或者是任何可以用文本表达的结构化数据（如markdown或latex中的表格、方程式等）。
**分词器**定义了如何将训练数据集中的文本转换为数字（因为模型是一个数学函数，因此需要数字作为输入）。分词是通过将文本转换为称为token的子单元来完成的（具体是单词、子词还是字符，取决于分词方法）。分词器的词汇量大小表示它所知道的不同token的数量，通常在32k到200k之间。一个数据集的大小通常通过其包含的**token数量**进行衡量，一旦被拆分为这些独立的“原子化”单元序列，如今范围从数千亿个token到数万亿个token！
**训练超参数**然后定义模型如何训练。参数应该改变多少以适应每个新示例？模型应该多快更新？
一旦选择了这些参数，您只需1）大量计算能力来训练模型，2）有能力（且友善）的人员来运行和监控训练。训练本身包括实例化架构（在用于训练的硬件上创建矩阵）并使用上述超参数在训练数据集上运行训练算法。结果是一组模型**权重**。这些是学习后的模型参数，也是大多数人在讨论访问开放预训练模型时所指的内容。然后这些权重可以用于**推理**，即对新输入进行预测，例如生成文本。
预训练的大语言模型在预训练之后也可以针对特定任务进行专门化或适配，特别是当权重公开发布时。然后它们通过一个称为**微调**的过程，被用作用例和应用的起点。微调涉及在一个不同的——通常更专业且较小的——数据集上对模型进行额外的训练步骤，以优化其用于特定应用。即便此步骤在计算能力需求方面有成本，但通常比从头开始训练模型要便宜得多，无论是在经济上还是在环保方面。这也是高质量开源预训练模型引人瞩目的原因之一，因为即使从业者只有有限的计算预算，它们也可以被社区自由使用和构建。
## 🗝️ 2022年，从规模竞赛到数据竞赛

在2023年之前，社区有哪些开源模型？
到2022年初，机器学习的趋势是，模型越大（即参数越多），其性能就越好。特别是似乎超过某些规模门槛的模型在功能上会有显著提升，这两个概念被称为“涌现能力”和“缩放法则”。2022年发布的开源预训练模型家族大多遵循这一范式。 1\. \[BLOOM\](https://huggingface.co/papers/2211.05100) (BigScience 大型开源科学公益多语言模型) BLOOM是BigScience发布的模型家族，由包括面向60个国家和250个机构的1000名研究人员在内的协作努力，并由Hugging Face协调，与法国组织GENCI和IDRIS合作开发。这些模型使用仅解码器Transformer，进行了轻微修改（嵌入后归一化，\[^1\]以及使用ALiBi位置嵌入\[^2\]）。这个家族中最大的模型是一个176B参数模型，训练于46种人类语言和13种编程语言的350B token的多语言数据上。大部分训练数据已被发布，并且其来源、筛选和处理的详细信息已公布。它是迄今为止最大的开源大规模多语言模型。
1. [OPT](https://huggingface.co/papers/2205.01068) (开放预训练Transformer) OPT[模型](https://huggingface.co/facebook/opt-66b)家族由Meta发布。这些模型使用仅解码器的Transformer架构，遵循GPT-3论文中的技巧（特定的权重初始化，预归一化），并对注意力机制做了一些改进（交替的密集和局部带状注意力层）。该家族中最大的模型是一个具有1750亿参数的模型，使用来自大多数公共来源的数据（书籍、通过Reddit的社交数据、新闻、维基百科和其他各种互联网来源）训练了1800亿tokens。该模型家族的性能可与GPT-3模型相媲美，使用编码优化以减少计算强度。
1. [GLM-130B](https://huggingface.co/papers/2210.02414) (通用语言模型)[GLM-130B](https://huggingface.co/THUDM/glm-roberta-large)由清华大学和智谱.AI发布。它采用了完整的Transformer架构，并进行了一些更改（使用DeepNorm的后层归一化，旋转嵌入）。这个有130B参数的模型是在400B个英文和中文互联网数据（The Pile，悟道语料库和其他中文语料库）上训练的。它的性能也与GPT-3模型相当。
1. 更小或更专业的开放式大语言模型也发布了，主要用于研究目的：Meta公司发布了[Galactica](https://huggingface.co/papers/2211.09085)系列，其大语言模型参数多达[1200亿](https://huggingface.co/facebook/galactica-120b)，在1060亿科学文献tokens上进行了预训练；EleutherAI发布了[GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)模型，该模型是完全开源的（包括架构、权重和数据），是一个解码器Transformer模型，经过5000亿tokens训练（使用RoPE并对注意力和初始化进行了一些更改），提供了完整的人工制品用于科学研究。
这些巨大的模型令人兴奋，但运行成本也非常高！在进行推理（从模型计算预测）时，模型需要加载到内存中，但一个拥有1000亿参数的模型通常需要220GB的内存来加载（我们在下面解释这一过程），这非常庞大，大多数组织和从业者无法获得如此多的内存！
然而，在2022年3月，DeepMind发布了一篇[新论文](https://huggingface.co/papers/2203.15556)，研究了在给定计算预算的情况下，token与模型参数的最佳比例是什么。换句话说，如果你只有金额X用来进行模型训练，那么模型和数据的相应大小应该是多少？作者发现，对于花费在大语言模型上的平均计算预算来说，模型应该更小，但需要大量更多的数据进行训练。他们自家的模型Chinchilla（不是开源的）拥有700亿参数（大约是上述模型三分之一的大小），但在1.4万亿token的数据上进行了训练（数据量是上述模型的3到4倍）。它的表现与其更大规模的开源或闭源对手相似或更好。
这一范式转变，虽然可能在封闭的实验室中已经为人所知，但在开放科学界引起了轰动。
## 🌊 2023年，开放发布之年

### 小型大语言模型的崛起

2023年，解码器风格的transformer如潮水般涌现，每月、甚至每周或每天都有新的预训练模型问世：2月的LLaMA（Meta公司），4月的StableLM（StabilityAI公司）和Pythia（Eleuther AI公司），5月的MPT（MosaicML公司），6月的X-GEN（Salesforce公司）和Falcon（TIIUAE公司），7月的Llama 2（Meta公司），8月的StableLM v2（StabilityAI公司），9月的Qwen（阿里巴巴公司）和Mistral（Mistral.AI公司），11月的Yi（01-ai公司），以及12月的DeciLM（Deci公司）、Phi-2和SOLAR（Upstage公司）。
所有这些发布a) 包括模型权重（在不同程度开放许可下）和b) 在较小规模的模型上表现良好（参数在3B到70B之间），因此，它们立即被社区采用。几乎所有这些模型都使用解码器Transformer架构，并进行了各种调整（ALiBi或RoPE，RMS预归一化，SwiGLU），以及对注意力函数的一些改动（Flash-Attention, GQA, 滑动窗口）和不同代码库实现以优化训练或推理速度。这些调整可能在一定程度上影响性能和训练速度；然而，由于所有架构都已与权重一起公开，剩下的核心差异是训练数据和模型许可。
这个系列的第一个模型家族是由 Meta AI 发布的[LLaMA](https://huggingface.co/papers/2302.13971)家族。研究人员的明确目标是训练一组不同大小的模型，以在给定的计算预算下达到最佳性能。研究团队第一次明确决定考虑不仅是训练预算，还包括推理成本（在实现给定性能目标的情况下，运行模型推理需要多少成本）。从这个角度来看，他们决定在更多数据和更多步骤上训练更小的模型，这样做比通常的方法达到更高的性能（权衡点是计算效率的训练）。Llama 1 家族中最大的模型是一个拥有 650 亿参数的模型，训练在 1.4T 个 token 上，而较小的模型（即分别拥有 60 亿和 130 亿参数）则训练在 1T 个 token 上。小型的 130 亿参数的 LLaMA 模型在大多数基准上表现优于 GPT-3，而最大的 LLaMA 模型在推出时是最先进的。不过，这些权重以非商业许可证发布，限制了社区的采用。
Pythia 模型由开源非营利实验室 Eleuther AI 发布，这是一套不同尺寸的大语言模型，基于完全公共的数据进行训练，旨在帮助研究人员理解大语言模型训练的不同步骤。
MosaicML公司发布的[MPT模型](https://www.mosaicml.com/blog/mpt-7b)在几个月后发布，性能非常接近，但许可允许商业使用，并详细介绍了它们的训练数据混合。第一款MPT模型是[7B模型](https://huggingface.co/mosaicml/mpt-7b)，随后在6月份推出了30B版本，两者都在1兆个Token的英语和代码上训练（使用来自C4、CommonCrawl、The Stack、S2ORC的数据）。
MPT 模型很快就被 TIIUAE 释出的来自 Falcon 系列的 7 和 30B 模型所跟随，这些模型是在 1 到 1.5T 英文和代码 tokens 上训练的（RefinedWeb、Project Gutemberg、Reddit、StackOverflow、Github、arXiv、Wikipedia 等来源）——在今年晚些时候，还发布了一个巨大的 180B 模型。Falcon 模型、数据和训练过程在技术报告和后来的研究论文中进行了详细介绍。
继承自GPT-Neo-X模型，StabilityAI发布了StableLM-Base-Alpha模型，这是一系列小型（3B和7B）预训练模型，使用了一个实验数据集中的1.5万亿个Token，该数据集基于ThePile构建，随后是一个包含RefinedWeb、RedPajama、ThePile和未公开的内部数据集的数据混合的v2系列，最后是一个非常小的3B模型，StableLM-3B-4e1T，配有一份详细的技术报告。
之前的模型大多公开数据，而从那时起，随后的版本几乎没有提供关于用于训练模型的数据的信息，因此他们的努力无法被复制——然而，他们通过发布的权重为社区提供了起点。
夏初, Salesforce推出了X-Gen模型，7B参数的模型，在1.5T的“自然语言和代码”token上进行训练，分几个步骤进行，遵循数据调度系统（并不是所有数据同时引入模型）。
X-Gen在某种程度上被Meta显而易见的全新LLaMA-2家族所掩盖，这一系列从7B到70B的模型是在2T的、来自“公开可用来源”的tokens上训练的，具有宽松的社区许可，并经过广泛的人类偏好微调过程（强化学习人类反馈），即所谓的对齐程序。
几个月后，新成立的初创公司 Mistral 推出了第一个所谓的 Mistral-7B 模型，该模型是在从 "开放网络" 中提取的数据的不限数量的 tokens 上训练的。2023 年年底，随着 Mistral 的第二个更大模型 (Mixtral 8x7B) 的发布，非常繁忙。Deci.AI 发布了一个令人印象深刻的首个模型，称为 DeciLM，此外还有来自 Upstage 的更大模型合并，称为 SOLAR，这些模型也是在未公开的数量和来源的数据上训练的。所有这些模型在排行榜和开放基准上保持稳定增长。
与此同时，2023年末的一个显著事件是中国训练并公开发布的表演和多个模型的兴起。发布了两个英汉双语模型系列：来自阿里巴巴的Qwen，参数为7到70B的[模型](https://huggingface.co/papers/2309.16609)，在2.4T tokens上训练，以及来自01-AI的Yi，参数为6到34B的[模型](https://huggingface.co/01-ai/Yi-34B)，在3T tokens上训练。这些模型的表现不仅在[开放 LLM 排行榜](https://huggingface.co/spaces/HuggingFaceH4/open%5Fllm%5Fleaderboard)上领先于以前的模型，还在一些最困难的基准如[Skill-Mix](https://huggingface.co/papers/2310.17567)上表现突出。另一个在2023年底的强劲竞争者是来自[DeepSeek AI](https://huggingface.co/deepseek-ai)的DeepSeek编码模型，从头开始在2T tokens上训练，由87%的代码和13%的自然语言组成，涵盖英语和汉语（主要是代码模型）。

### 对话模型无处不在

与2022年相比，几乎所有在2023年发布的预训练模型都同时提供了预训练版本和通过现有的几种方法之一进行对话微调的版本。虽然将模型适应聊天环境的方法在2022年及之前已经被开发，但这些技术在2023年才真正被广泛采用，突显了普通公众对这些聊天模型的日益增多的使用，以及通过与它们聊天进行的日益增多的人工评价（“氛围检测”评价）。我们在此详述了最知名的一些将预训练模型适应聊天的方式，但实际上存在许多变体！
**基于聊天的微调**是一种监督微调的变体，其中注释数据是聊天数据（多轮对话类似的数据，就像您在社交媒体上找到的一样），您在其上对模型进行微调。您使用与训练模型时相同的技术：对于解码器Transformer，教模型逐个预测下一个词（称为自回归方法）。
**指令微调** (IFT) 遵循相同的方法，但使用指令数据集，这些数据集包含一系列类查询的提示词加上答案（如果需要，还可以包括其他输入）。这些数据集教会模型如何遵循指令，可以是人工或大语言模型生成的。使用大规模模型输出的合成数据集（例如，从 GPT-4 的生成，可能是从指令或用户与该模型之间的交互生成的数据集）是实现指令和聊天微调的方法之一。这通常被称为`蒸馏`，因为它涉及从高性能模型中提取知识以训练或微调一个较小的模型。
